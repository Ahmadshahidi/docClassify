---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r one, message=FALSE, warning=FALSE}
require(stringr)
require(tm.plugin.webmining)
require(tm)
require(SnowballC)
require(RTextTools)
require(R.utils)
require(utils)
require('caret')
#File List and Name Shuffle
# list of spam files
spam_files <-list.files(path="spam/", full.names=T, recursive=FALSE)
# list of ham files
ham_files <- list.files(path="easy_ham_2/",full.names=T, recursive=FALSE)
# concatenate ham and spam file lists
ham_spam <- c(ham_files,spam_files)
#shuffle file names
set.seed(2020)
ham_spam <- sample(ham_spam,length(ham_spam))


# is required for extractHTMLStrip() function to works.
Sys.setlocale("LC_ALL", "C")
#--------------------------------------


#Cleaning Scripts
# function to find first blank line in email.
# using this function to estimate where email body begins.
find_blank_line <-function(x){
  for (i in 1:length(x)){
    if (str_detect(x[i],"^[:space:]*$")){
      result <- i
      return(i) 
    }
  }
}

# set up variables for loop
n <- 0
if(exists('email_corpus')){rm(email_corpus)} 

# loop through each email
for (i in 1:length(ham_spam)){
  tmp <- readLines(ham_spam[i])
  
  # remove email header
  beg <- find_blank_line(tmp)+1
  end <- length(tmp)
  tmp <- tmp[beg:end]
  
  # remove HTML tags
  if(extractHTMLStrip(tmp)!=""){
    tmp <- extractHTMLStrip(tmp)
  }
  
  # remove URL links, punctuation, numbers, newlines, and misc symbols
  tmp <- unlist(str_replace_all(tmp,"[[:punct:]]|[[:digit:]]|http\\S+\\s*|\\n|<|>|=|_|-|#|\\$|\\|"," "))
  
  # remove extra whitespace
  tmp <- str_trim(unlist(str_replace_all(tmp,"\\s+"," ")))                           
  
  tmp <- str_c(tmp,collapse="")
  
  # Add emails to corpus, and include spam/ham category information
  if (length(tmp)!=0){
    n <- n + 1
    tmp_corpus <- VCorpus(VectorSource(tmp))
    ifelse(!exists('email_corpus'), email_corpus <- tmp_corpus, email_corpus <- c(email_corpus,tmp_corpus))
    meta(email_corpus[[n]], "spam_ham") <- ifelse(str_detect(ham_spam[i],"spam"),1,0)
    
  }
}    

#Corpus Scubbing
#Initial DTM
dtm <- DocumentTermMatrix(email_corpus)
dtm
#Intermediate DTM
# transform all words in corpus to lower case
email_corpus_mod <- tm_map(email_corpus, content_transformer(tolower))

# remove all stop words (e.g. "i", "me", "she", etc.)
email_corpus_mod <- tm_map(email_corpus_mod,removeWords, words = stopwords("en"))

# stem words: cut certain terms down to word root
email_corpus_mod <- tm_map(email_corpus_mod, stemDocument)

dtm <- DocumentTermMatrix(email_corpus_mod)
dtm
#Final DTM
dtm <- removeSparseTerms(dtm,1-(10/length(email_corpus_mod)))
dtm

###Classifier Models
# create spam label vector for each email which indiciates actual status of "spam" or "not spam" 

spam_labels_prelim <- unlist(meta(email_corpus_mod,"spam_ham"))

spam_labels <- c(rep(NA,length(email_corpus_mod)))

for (i in 1:length(email_corpus_mod)){
  spam_labels[i] <- spam_labels_prelim[[i]]
}


#Set Up Models
# number of emails in corpus
N <- length(spam_labels)
trainpartition <- round(.80 * N)
# set up model container; 80/20 split between train and test data
container <- create_container(
  dtm,
  labels = spam_labels,
  trainSize = 1:trainpartition,
  testSize = (trainpartition+1):N,
  virgin = FALSE
)

svm_model <- train_model(container, "SVM")
tree_model <- train_model(container, "TREE")
maxent_model <- train_model(container, "MAXENT")
glm_model <- train_model(container, "GLMNET")

svm_out <- classify_model(container, svm_model)
tree_out <- classify_model(container, tree_model)
maxent_out <- classify_model(container, maxent_model)
glm_out <- classify_model(container,glm_model)


########################################Model Performance
# create lables:  actual classification, then model classification
# for three models on test data

labels_out <- data.frame(
  correct_label = spam_labels[(trainpartition+1):N],
  svm = as.character(svm_out[,1]),
  tree = as.character(tree_out[,1]),
  maxent = as.character(maxent_out[,1]),
  glm = as.character(glm_out[,1]),
  stringAsFactors = F)

#func to call confusionMatrix 
makematrix <- function(col) {
  confusionMatrix(table(labels_out[[col]], labels_out$correct_label), positive= "0")
}

# create all 
svmconf <- makematrix('svm')
treeconf <- makematrix('tree')
glmconf <- makematrix('glm')
maxentconf <- makematrix('maxent')

#Plot: Confusion Matrix
par(mfrow=c(2,2))
fourfoldplot(svmconf$table, color = c("#B22222", "#2E8B57"), main="SVM")
fourfoldplot(treeconf$table, color = c("#B22222", "#2E8B57"), main="Tree")
fourfoldplot(glmconf$table, color = c("#B22222", "#2E8B57"), main="GLM")
fourfoldplot(maxentconf$table, color = c("#B22222", "#2E8B57"), main="MaxEnt")


#####################Comparison Table: F1 Score
#setup df
eval <- data.frame(treeconf$byClass, 
                   svmconf$byClass,
                   glmconf$byClass,
                   maxentconf$byClass)

eval <- data.frame(t(eval))

#calc FScore
precision <- eval$Pos.Pred.Value
recall <- eval$Sensitivity
eval$Fscore <- 2 * ((precision * recall) / (precision + recall))  

# manipulate results DF
eval <- eval[,c(1:3, 9,12)]
row.names(eval) <- c("Tree", "SVM", "GLM", "MaxEnt")
eval <- eval[order(eval$Fscore, decreasing=TRUE),]

knitr::kable(eval)

```

There was a problem that Ramandeep notice, and later I finally understood what he meant. It seems that there is no tool here to apply a trained model to new cases and classify them based what we have learned. I don't think that is a problem at all for two reasons. First, we can easily adopt the reasulting model so that we can apply it on the classification of new documents (see below for an example). Second, this is the building and testing stage in which we only look to try different models and compare the results and choose the one we finally persive the best. When we go to impelementation stage then we need to have a model that will be used to classify new documents. As I understand implementation won't be in R and therefore non of these package will be used. This approche gives us very easy to use tools to build, test, and compare models. Therefore I think it is wise to use it.

Here is an example how to use a model built using *tm* and *RtextTools* to score new documents.


```{r}
#First define a prep function to clean the documents.
prep_doc <- function(tmp){

  # remove email header
  beg <- find_blank_line(tmp)+1
  end <- length(tmp)
  tmp <- tmp[beg:end]
  
  # remove HTML tags
  if(extractHTMLStrip(tmp)!=""){
    tmp <- extractHTMLStrip(tmp)
  }
  
  # remove URL links, punctuation, numbers, newlines, and misc symbols
  tmp <- unlist(str_replace_all(tmp,"[[:punct:]]|[[:digit:]]|http\\S+\\s*|\\n|<|>|=|_|-|#|\\$|\\|"," "))
  
  # remove extra whitespace
  tmp <- str_trim(unlist(str_replace_all(tmp,"\\s+"," ")))                           
  
  tmp <- str_c(tmp,collapse="")
  tmp
}    

#Here we define a Lable function that labels new documents.
Label <- function(doc_to_label, train_dtm, model)
{
  # Using creat_matrix will make a one row DTM consistent with the one that the model has been trained on
  #After that we can make a container with only one row. Notice testSize = 1
  #Third line applies the given model on this document.
  predict_matrix <- create_matrix(doc_to_label, originalMatrix = train_dtm)
  predict_container <- create_container(predict_matrix, labels = NULL, trainSize = NULL ,testSize = 1, virgin = FALSE) 
  return(classify_model(predict_container, model))
}
```

Let's use the function. I created a folder *To_label* and cut pasted a few from the original emails and put them there. Then I trained the model on the rest and used it to label those in the *To_label* folder.

```{r labeling}
docList <- list.files("To_label/", full.names=T, recursive=FALSE)
doc1 <- readLines(docList[1])
doc1 <- prep_doc(doc1)

Label(doc1,dtm,svm_model)
```
This means that SVM give doc1 a label of 0 (spam) with probability of 0.9978919. 